{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import randint\n",
    "from torch import nn, optim\n",
    "import torch \n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from scores.score_logger import ScoreLogger\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.state = np.zeros((3,3), dtype=int)\n",
    "        self.player = \"1\"\n",
    "        self.winner = None\n",
    "        self.done = False\n",
    "\n",
    "    def step(self, action):\n",
    "        self.state[action] = self.player\n",
    "        self.player = -self.player\n",
    "        self.winner = self.get_winner(self.state)\n",
    "        self.done = self.winner is not None or len(self.get_legal_actions()) == 0\n",
    "        return self.state, self.winner, self.done\n",
    "\n",
    "    def get_winner(self, state):\n",
    "        for row in state:\n",
    "            if abs(np.sum(row)) == len(row):\n",
    "                return np.sum(row) / len(row)\n",
    "        for col in state.transpose():\n",
    "            if abs(np.sum(col)) == len(col):\n",
    "                return np.sum(col) / len(col)\n",
    "        if abs(np.sum(np.diag(state))) == len(np.diag(state)):\n",
    "            return np.sum(np.diag(state)) / len(np.diag(state))\n",
    "        if abs(np.sum(np.diag(np.fliplr(state)))) == len(np.diag(np.fliplr(state))):\n",
    "            return np.sum(np.diag(np.fliplr(state))) / len(np.diag(np.fliplr(state)))\n",
    "                \n",
    "        return None\n",
    "\n",
    "    def get_legal_actions(self, state):\n",
    "        return np.argwhere(state == 0)\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.zeros((3,3), dtype=int)\n",
    "        self.player = \"1\"\n",
    "        self.winner = None\n",
    "        self.done = False\n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloTreeSearch:\n",
    "    def __init__(self, env, tree, n_iterations=50, depth=15, exploration_constant=5.0):\n",
    "\n",
    "        self.n_iterations = n_iterations\n",
    "        self.depth = depth\n",
    "        self.exploration_constant = exploration_constant\n",
    "        self.total_simulation_count = 0\n",
    "        self.env = env\n",
    "        self.tree = tree.add_node((), (0, ), env.state, player=1)\n",
    "\n",
    "    def selection(self): \n",
    "\n",
    "        leaf_node_found = False\n",
    "        leaf_node_id = (0,)\n",
    "\n",
    "        while not leaf_node_found:\n",
    "\n",
    "            node = self.tree[leaf_node_id]\n",
    "            \n",
    "            if len(node.actions) == 0:\n",
    "                leaf_node_id = node.id\n",
    "                leaf_node_found = True\n",
    "            else: \n",
    "                UCB = -100\n",
    "                for action in node.actions:\n",
    "                    child = self.tree[node.id + (action,)]\n",
    "\n",
    "                    # prevent divide by zero where child.visits == 0\n",
    "                    n = child.visits\n",
    "                    if n == 0:\n",
    "                        n = 1e-4\n",
    "\n",
    "                    exploitation_value = child.reward / child.visits\n",
    "                    exploration_value  = np.sqrt(np.log(self.total_simulation_count)/child.count)\n",
    "                    uct_value = exploitation_value + self.exploration_constant * exploration_value\n",
    "\n",
    "                    if uct_value > maximum_uct_value:\n",
    "                        maximum_uct_value = uct_value\n",
    "                        leaf_node_id = child.id\n",
    "\n",
    "        depth = len(leaf_node_id) # as node_id records selected action set\n",
    "        # print('leaf node found: ', leaf_node_found)\n",
    "        # print('n_child: ', n_child)\n",
    "        # print('selected leaf node: ')\n",
    "        # print(self.tree[leaf_node_id])\n",
    "        return leaf_node_id, depth\n",
    "                    \n",
    "\n",
    "    def expansion(self, leaf_node_id):\n",
    "        '''\n",
    "        create all possible outcomes from leaf node\n",
    "        in: tree, leaf_node\n",
    "        out: expanded tree (self.tree),\n",
    "             randomly selected child node id (child_node_id)\n",
    "        '''\n",
    "        leaf_node = self.tree[leaf_node_id]\n",
    "        winner = self.env.get_winner(leaf_node.state)\n",
    "        possible_actions = self.env.get_legal_actions(leaf_node.state)\n",
    "\n",
    "        child_node_id = leaf_node.id # default value in case of game termination\n",
    "        if winner is None:\n",
    "            '''\n",
    "            when leaf state is not terminal state\n",
    "            '''\n",
    "            childs = []\n",
    "            for action_set in possible_actions:\n",
    "                action, action_idx = action_set\n",
    "                state = deepcopy(leaf_node.state)\n",
    "\n",
    "                if leaf_node.player == '1':\n",
    "                    next_turn = '-1'\n",
    "                    state[action] = 1\n",
    "                else:\n",
    "                    next_turn = '1'\n",
    "                    state[action] = -1\n",
    "\n",
    "                #Node id is a tuple of action set\n",
    "                child = self.tree.add_node(leaf_node_id, action_idx, state)\n",
    "                self.tree[leaf_node_id].actions.append(action_idx)\n",
    "\n",
    "            rand_idx = np.random.randint(low=0, high=len(childs), size=1)\n",
    "            # print(rand_idx)\n",
    "            # print('childs: ', childs)\n",
    "            child_node_id = childs[rand_idx[0]]\n",
    "\n",
    "        return child_node_id\n",
    "\n",
    "    \n",
    "    def simulation(self, child_node_id):\n",
    "        '''\n",
    "        simulate game from child node's state until it reaches the resulting state of the game.\n",
    "        in:\n",
    "        - child node id (randomly selected child node id from `expansion`)\n",
    "        out:\n",
    "        - winner ('o', 'x', 'draw')\n",
    "        '''\n",
    "        self.total_simulation_count += 1\n",
    "\n",
    "        #Deep copy so as to not update the actual node\n",
    "        state = deepcopy(self.tree[child_node_id]['state'])\n",
    "        previous_player = deepcopy(self.tree[child_node_id]['player'])\n",
    "        anybody_win = False\n",
    "\n",
    "        while not anybody_win:\n",
    "            winner = self.env.get_winner(state)\n",
    "            if winner is not None:\n",
    "                # print('state')\n",
    "                # print(state)\n",
    "                # import matplotlib.pyplot as plt\n",
    "                # plt.figure(figsize=(4.5,4.56))\n",
    "                # plt.pcolormesh(state, alpha=0.6, cmap='RdBu_r')\n",
    "                # plt.grid()\n",
    "                # plt.axis('equal')\n",
    "                # plt.gca().invert_yaxis()\n",
    "                # plt.colorbar()\n",
    "                # plt.title('winner = ' + winner + ' (o:1, x:-1)')\n",
    "                # plt.show()\n",
    "                anybody_win = True\n",
    "            else:\n",
    "                possible_actions = self.env.get_legal_actions(state)\n",
    "                # randomly choose action for simulation (= random rollout policy)\n",
    "                rand_idx = np.random.randint(low=0, high=len(possible_actions), size=1)[0]\n",
    "                action, _ = possible_actions[rand_idx]\n",
    "\n",
    "                if previous_player == '1':\n",
    "                    current_player = '-1'\n",
    "                    state[action] = 1\n",
    "                else:\n",
    "                    current_player = '1'\n",
    "                    state[action] = -1\n",
    "\n",
    "                previous_player = current_player\n",
    "        return winner\n",
    "\n",
    "    def backprop(self, child_node_id, winner):\n",
    "        player = deepcopy(self.tree[(0,)].player)\n",
    "\n",
    "        if winner == 'draw':\n",
    "            reward = 0\n",
    "        elif winner == player:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        node_id = child_node_id\n",
    "        while (True):\n",
    "            self.tree[node_id].visits += 1\n",
    "            self.tree[node_id].reward += reward\n",
    "            self.tree[node_id].q = self.tree[node_id].reward / self.tree[node_id].visits\n",
    "            parent_id = self.tree[node_id].parent\n",
    "            if parent_id == (0,):\n",
    "                self.tree[parent_id].visits += 1\n",
    "                self.tree[parent_id].reward += reward\n",
    "                self.tree[parent_id].q = self.tree[parent_id].reward / self.tree[parent_id].visits\n",
    "                break\n",
    "            else:\n",
    "                node_id = parent_id\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node: \n",
    "    def __init__ (self, parent_id, action, state, player): \n",
    "        self.id = parent_id + (action,)\n",
    "        self.parent = parent_id\n",
    "        self.state = state\n",
    "        self.actions = []\n",
    "        player = player\n",
    "        self.reward = 0\n",
    "        self.visits = 0\n",
    "        self.q = 0\n",
    "\n",
    "class Tree: \n",
    "    def __init__ (self, state): \n",
    "        self.tree = {}\n",
    "    \n",
    "    def add_node(self, id, state, player=None):\n",
    "        self.tree[id] = Node(id, state, player)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToe()\n",
    "score_logger = ScoreLogger(ENV_NAME)\n",
    "\n",
    "# Reset the environment and get the first state\n",
    "state, info = env.reset(seed=46, return_info=True)\n",
    "\n",
    "# Initialize the tree\n",
    "tree = Tree(state)\n",
    "\n",
    "# Create the agent\n",
    "MONTY_CARLO_TREE_SEARCH = MonteCarloTreeSearch(env, tree, num_simulations=10, c=1.0)\n",
    "\n",
    "run = 0 # run is the number of episodes\n",
    "while run < 100:\n",
    "    run += 1\n",
    "    env.reset()\n",
    "    step = 0\n",
    "    while not env.done:\n",
    "        step += 1\n",
    "        # Get the action from the agent\n",
    "        action = DQN_AGENT.act(state)\n",
    "        # Step the environment and get the next state, reward, and done flag\n",
    "        state, reward, done = env.step(action)\n",
    "        # Update the agent with the new experience\n",
    "        MONTY_CARLO_TREE_SEARCH.update(state, action, reward, done)\n",
    "        # Render the environment\n",
    "        env.render()\n",
    "        # If the episode is done, break the loop\n",
    "        if done:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('aiGym-3-9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d989930d2ce6478f400bece301e290be5d0537f88c7447ba48889ddf48d55e98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
