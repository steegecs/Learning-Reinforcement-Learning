{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5.1 Consider the diagrams on the right in Figure 5.1. Why does the estimated\n",
    "value function jump up for the last two rows in the rear? Why does it drop o↵ for the\n",
    "whole last row on the left? Why are the frontmost values higher in the upper diagrams\n",
    "than in the lower?\n",
    "- It jumps in the rear, because the player has a high number, the dealer has a high probability of busting if he or she tries to get a higher number. It drops on on the last row on the left because the dealing is holding an ace. This gives the dealer more action flexibility and a higher probability of winning because it can also be paired with a 10 valued card. The frontmost values are higher in the upper diagram, because the ace is a very good and flexible card that increases your chance even with mid-low value hands. The ace being present can give you multiple chances at reaching the 20 or 21 stick policy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5.2 Suppose every-visit MC was used instead of first-visit MC on the blackjack\n",
    "task. Would you expect the results to be very di↵erent? Why or why not?\n",
    "- It depends on if you are sampling from the same distribution of cards from one card to the next. If you are, then no, it does not make a difference. But, if it is the case the drawing certain cards changes the draw distribution, then yes it would be different. \n",
    "- Monte Carlo methods do not **bootstrap** as was done for the DP methods. This means the the evaluation of a state does not depend on the previous valuation of another state. \n",
    "- **In particular, note that the c\n",
    "omputational expense of estimating the value of a single state is independent of the number of states. This can make Monte Carlo methods particularly attractive when one requires the value of only one or a subsetof states.**\n",
    "-  One can generate many sample episodes starting from the states of interest,\n",
    "averaging returns from only these states, ignoring all others. This is a third advantage\n",
    "Monte Carlo methods can have over DP methods (after the ability to learn from actual\n",
    "experience and from simulated experience)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5.3 What is the backup diagram for Monte Carlo estimation of q⇡?\n",
    "\n",
    "- Notice how it does not expand out like a tree for every state, this is because, again, it evaluations like state action pairs. \n",
    "\n",
    "![](images/Monte-Carlo-Back-Diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5.4 The pseudocode for Monte Carlo ES is inecient because, for each state–\n",
    "action pair, it maintains a list of all returns and repeatedly calculates their mean. It would\n",
    "be more ecient to use techniques similar to those explained in Section 2.4 to maintain\n",
    "just the mean and a count (for each state–action pair) and update them incrementally.\n",
    "Describe how the pseudocode would be altered to achieve this.\n",
    "\n",
    "- Instead of appending each G to returns, just keep track of the current count and means in a tuple in this data structure, then you can use the incremental mean update formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5.5 Consider an MDP with a single nonterminal state and a single action\n",
    "that transitions back to the nonterminal state with probability p and transitions to the\n",
    "terminal state with probability 1p. Let the reward be +1 on all transitions, and let\n",
    " = 1. Suppose you observe one episode that lasts 10 steps, with a return of 10. What\n",
    "are the first-visit and every-visit estimators of the value of the nonterminal state?\n",
    "- First Visit = p\n",
    "- Many Visit = p + p ^2 + p^3 ... + p^10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5.6 What is the equation analogous to (5.6) for action values Q(s, a) instead of\n",
    "state values V (s), again given returns generated using b?\n",
    "\n",
    "![](images/unnamed.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5.7 In learning curves such as those shown in Figure 5.3 error generally decreases\n",
    "with training, as indeed happened for the ordinary importance-sampling method. But for\n",
    "the weighted importance-sampling method error first increased and then decreased. Why\n",
    "do you think this happened?\n",
    "- Weighted importance sampling is biased toward the value estimated under policy (b). This policy is random, so until it has done enough exploration to starts discovering better trajecotries, the error increases. But once it has explored enough, it can see more of the state space and better approximate the target policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5.8 The results with Example 5.5 and shown in Figure 5.4 used a first-visit MC method. Suppose that instead an every-visit MC method was used on the same problem. Would the variance of the estimator still be infinite? Why or why not?\n",
    "- The rewards are given out at termination so the variance of this reward is equal to one. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56941cacf15e8b05765996006082865469347c2b4cdce983108d1335de8b4245"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
