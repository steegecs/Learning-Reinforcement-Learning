{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5.1 Consider the diagrams on the right in Figure 5.1. Why does the estimated\n",
    "value function jump up for the last two rows in the rear? Why does it drop o↵ for the\n",
    "whole last row on the left? Why are the frontmost values higher in the upper diagrams\n",
    "than in the lower?\n",
    "- It jumps in the rear, because the player has a high number, the dealer has a high probability of busting if he or she tries to get a higher number. It drops on on the last row on the left because the dealing is holding an ace. This gives the dealer more action flexibility and a higher probability of winning because it can also be paired with a 10 valued card. The frontmost values are higher in the upper diagram, because the ace is a very good and flexible card that increases your chance even with mid-low value hands. The ace being present can give you multiple chances at reaching the 20 or 21 stick policy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5.2 Suppose every-visit MC was used instead of first-visit MC on the blackjack\n",
    "task. Would you expect the results to be very di↵erent? Why or why not?\n",
    "- It depends on if you are sampling from the same distribution of cards from one card to the next. If you are, then no, it does not make a difference. But, if it is the case the drawing certain cards changes the draw distribution, then yes it would be different. \n",
    "- Monte Carlo methods do not **bootstrap** as was done for the DP methods. This means the the evaluation of a state does not depend on the previous valuation of another state. \n",
    "- **In particular, note that the c\n",
    "omputational expense of estimating the value of a single state is independent of the number of states. This can make Monte Carlo methods particularly attractive when one requires the value of only one or a subsetof states.**\n",
    "-  One can generate many sample episodes starting from the states of interest,\n",
    "averaging returns from only these states, ignoring all others. This is a third advantage\n",
    "Monte Carlo methods can have over DP methods (after the ability to learn from actual\n",
    "experience and from simulated experience)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5.3 What is the backup diagram for Monte Carlo estimation of q⇡?\n",
    "\n",
    "- Notice how it does not expand out like a tree for every state, this is because, again, it evaluations like state action pairs. \n",
    "\n",
    "![](images/Monte-Carlo-Back-Diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5.4 The pseudocode for Monte Carlo ES is inecient because, for each state–\n",
    "action pair, it maintains a list of all returns and repeatedly calculates their mean. It would\n",
    "be more ecient to use techniques similar to those explained in Section 2.4 to maintain\n",
    "just the mean and a count (for each state–action pair) and update them incrementally.\n",
    "Describe how the pseudocode would be altered to achieve this.\n",
    "\n",
    "- Instead of appending each G to returns, just keep track of the current count and means in a tuple in this data structure, then you can use the incremental mean update formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56941cacf15e8b05765996006082865469347c2b4cdce983108d1335de8b4245"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
