{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Concepts: \n",
    "- Solving RL problems without a complete model of the (environment Model-Free).\n",
    "    - No prior knowledge of the state\n",
    "    - Monte Carlo describes randomized algorithms. \n",
    "        - Used to describe randomly sampling episodes in our environment.\n",
    "        - Updates in GPI come from experience.\n",
    "    - Different than DP because you are generating episodes and only using the results from episodes to do GPI, whereas with DP, you are updating with respect to an environment that you have a complete representation of (Model).\n",
    "        - Does not \"look ahead\" like dynamic programming does. \n",
    "- Sampling Returns vs. Bellman Equation Bootstrapping.\n",
    "- First-visit vs. Many-visit.\n",
    "- Exploration / Eploitation Balance.\n",
    "- Exploring Starts.\n",
    "    - Environment can be initialized with every state-action pair. \n",
    "- e-soft policies.\n",
    "    - e-soft uses the epsilon value to provide a decision function for being greedy or exploring.\n",
    "- Off Policy learning with importance sampling.\n",
    "    - Target Policy vs. Behavior Policy\n",
    "    - Importance Sampling\n",
    "        - Importance Sampling Ratio\n",
    "        - Policies that are more likely under the target policy get weighted more for policy evaluation. \n",
    "        - Ordinary vs. Weighted Importance Sampling\n",
    "            - Ordanary is weighted by the length of the sequence\n",
    "            - Weighted is weighted by the same summation of the importance sample ratio. \n",
    "                - Does not cancel out because on the top you are multiplying by the return of at every state of the summation loop. \n",
    "\n",
    "- Incremental Updates\n",
    "    - Using efficient incremental updates of averages as also outlined in Chapter 2.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 5 Monte Carlo Methods\n",
    "- Require on experience to learn and do not need a complete description of the environment like Dynamic Programming policy evaluation and improvement algorithms did from the previous chapter. \n",
    "- Learning from actual experience is striking because it requires no prior knowledge of the environment’s dynamics, yet can still attain optimal behavior. Learning  from simulated experience is also powerful. \n",
    "-  To ensure that well-defined returns are available, here we define Monte Carlo methods only for episodic tasks. That is, we assume experience is divided into episodes, and that all episodes eventually terminate no matter what actions are  selected. Only on the completion of an episode are value estimates and policies changed\n",
    "- The term “Monte Carlo” is often used more broadly for any estimation method whose operation involves a significant random component. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 Monte Carlo Prediction \n",
    "- Value of a state is the expected return\n",
    "- One obvious way to compute is average returns. \n",
    "- First Visit and every-visit monte carlo methods. \n",
    "    - First visit only uses 1 first return for each state visited in an episode\n",
    "    - Every visit averages returns from re-visited states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 Monte Carlo Estimation of Action Values\n",
    "\n",
    "- The every-visit MC method estimates the value of a state–action pair as the average of the returns that have followed all the visits to it. The first-visit MC method averages the returns following the first time in each episode that the state was visited and the action was selected. These methods converge quadratically, as before, to the true expected values as the number of visits to each state–action pair approaches infinity.\n",
    "- The only complication is that many state–action pairs may never be visited. If ⇡ is a deterministic policy, then in following ⇡ one will observe returns only for one of the actions from each state. With no returns to average, the Monte Carlo estimates of the other actions will not improve with experience. \n",
    "- We need to **maintain exploration**. \n",
    "- One way to do this is by specifying that the episodes start in a **state–action pair**, and that every pair has a nonzero probability of being selected as the start. This guarantees that all state–action pairs will be visited an infinite number of times in the limit of an infinite number of episodes. We call this the assumption of **exploring starts**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 Monte Carlo Control\n",
    "- How is Monte Carlo used to aproximate optimal policies? \n",
    "    - Uses GPI\n",
    "        - Evaluate the policy then improve \n",
    "        - The value function is repeatedly altered to more closely approximate the value function for the current policy, and the policy is repeatedly improved with respect to the current value function\n",
    "        ![](images/GPI-Monte-Carlo.png)\n",
    "\n",
    "- Policy evaluation is done exactly as described in the preceding section. Many episodes are experienced, with the approximate action-value function approaching the true function asymptotically. \n",
    "\n",
    "- Policy improvement from 4.2 applies here as well. \n",
    "\n",
    "![](images/Monte-Carlo-Policy-Improvement.png)\n",
    "\n",
    "\n",
    "- We made two unlikely assumptions above in order to easily obtain this guarantee of convergence for the Monte Carlo method. One was that the episodes have exploringstarts, and the other was that policy evaluation could be done with an infinite number of episodes. To obtain a practical algorithm we will have to remove both assumptions. We postpone consideration of the first assumption until later in this chapter. For now we focus on the assumption that policy evaluation operates on an infinite number of episodes. This assumption is relatively easy to remove. In fact, the same issue arises even in classical DP methods such as iterative policy evaluation, which also converge only asymptotically to the true value function.\n",
    "\n",
    "- There is a second approach to avoiding the infinite number of episodes nominally required for policy evaluation, in which we give up trying to complete policy evaluation before returning to policy improvement. On each evaluation step we move the value function toward q⇡k , but we do not expect to actually get close except over many steps. We used this idea when we first introduced the idea of GPI in Section 4.6. One extreme form of the idea is value iteration, in which only one iteration of iterative policy evaluation is performed between each step of policy improvement. The in-place version of value iteration is even more extreme; there we alternate between improvement and evaluation steps for single states.\n",
    "\n",
    "- For Monte Carlo policy iteration it is natural to alternate between evaluation and improvement on an episode-by-episode basis.\n",
    "\n",
    "- This algorithm works by repeatedly generating episodes at a random state with a random probabilty > 0 of taking any action, looping through all steps in the episode and updating the average reward for a state (if it is the first visit in this episode), and updating the policy to be the action with the maximum average value from all previous episode visits. \n",
    "\n",
    "![](images/Monte-Carlo-ES.png)\n",
    "\n",
    "- Convergence to this optimal fixed point seems inevitable as the changes to the action-value function decrease over time, but has not yet been formally proved. In our opinion, this is one of the most fundamental open theoretical questions in reinforcement learning (for a partial solution, see Tsitsiklis, 2002)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4 Monte Carlo Control without Exploring Starts\n",
    "- How can we avoid the unlikely assumption of exploring starts? (i.e. that the starting state can be any state). \n",
    "- On-policy and 0ff-policy methods for ensuring all actions can be selected ad-infinitum. \n",
    "    - on-policy\n",
    "        - On policy methods are recursive, because improvement are made on the policy that implemented the action\n",
    "    - off-policy\n",
    "        - Policies that are generating data to improve another policy.  \n",
    "\n",
    "- On-Policy\n",
    "    - on-policy control methods the policy is generally soft, meaning that ⇡(a|s) > 0 for all s 2 S and all a 2 A(s)\n",
    "    - All actions have a greater than zero chance of being selected under the policy.  \n",
    "        - Remeber e-greedy policies? \n",
    "                \n",
    "    -  All nongreedy actions are given the minimal probability of selection\n",
    "    - ![](images/Non-greedy-probs.png)\n",
    "\n",
    "    - Greedy probabilities make up the rest after non-greedy policies get their epsilon based probabilities. \n",
    "    - ![](images/greedy-probs.png)\n",
    "\n",
    "    - e-soft policies have non-greedy action probabilities, but the greedy action is higher for all states and action.\n",
    "    - ![](images/e-soft.png)\n",
    "\n",
    "    - What is more greedy vs. less greedy? \n",
    "                    - Actions that have a higher probability for the greedy action are more greedy\n",
    "\n",
    "    - Fortunately, GPI does not require that the policy be taken all the way to a greedy policy, only that it be moved toward a greedy policy\n",
    "\n",
    "    - Below is an algorithm for GPI for Monte Carlo methods without the exploring starts assumption with on-Policy improvements.\n",
    "    - It works very similar to the algorithm with exploring started, except you don't have that assumption. You do not, however, always take the greedy option. if after updating the action values after the evaluation of an episode, you update the policy to be **more** greedy with respect to the optimal action for a state, and the other actions to be e-soft (I think thats the right way to put it?). Anyways, you give these ones the non-greedy probabilities based on epsilon.\n",
    "    \n",
    "    - ![](images/On-Policy-No-EC.png)\n",
    "\n",
    "\n",
    "\n",
    "    - The equation for the e-greedy actions at the end is structure this way so that the exploratory actions are 0'ed out, if it is e-greedy then the denominator re-normalizes it. \n",
    "    - ![](images/Policy-Improvement-Theorem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5 Off-Policy Prediction via Importance Sampling\n",
    "- All learning control methods face a dilemma: They seek to learn action values conditional on subsequent optimal behavior, but they need to behave non-optimally in order to explore all actions (to find the optimal actions).\n",
    "- One policy being learned and one generating data\n",
    "    - Target Policy\n",
    "    - Behavior Policy\n",
    "- In order to use episodes from b to estimate values for ⇡, we require that every action taken under ⇡ is also taken, at least occasionally, under b. That is, we require that ⇡(a|s) > 0 implies b(a|s) > 0. This is called the assumption of coverage.\n",
    "- In control, the target policy is typically the deterministic greedy policy with respect to the current estimate of the action-value function.\n",
    "\n",
    "- Almost all off policy methods use **importance** sampling. \n",
    "- We apply importance sampling to o↵-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the **importance-sampling ratio**.\n",
    "\n",
    "- NOTE: THIS IS A TRAJECTORY PROBABILITY\n",
    "\n",
    "![](images/Importance-Sampling.png)\n",
    "\n",
    "- G(t) comes from the Behavior Policy. We then weight it by the importance sampling ratio at that time. \n",
    "- 2 forms of Importance Sampling\n",
    "    - Ordinary Importance Sampling\n",
    "    \n",
    "    ![](images/Ordinary-Important-Sampling.png)\n",
    "    - Weighted Importance Sampling\n",
    "    \n",
    "    ![](images/Weighted-Importance-Sampling.png)\n",
    "\n",
    "\n",
    "- Formally, the di↵erence between the first-visit methods of the two kinds of importance\n",
    "sampling is expressed in their biases and variances. Ordinary importance sampling is\n",
    "unbiased whereas weighted importance sampling is biased (though the bias converges\n",
    "asymptotically to zero). On the other hand, the variance of ordinary importance sampling\n",
    "is in general unbounded because the variance of the ratios can be unbounded, whereas in\n",
    "the weighted estimator the largest weight on any single return is one. In fact, assuming\n",
    "bounded returns, the variance of the weighted importance-sampling estimator converges\n",
    "to zero even if the variance of the ratios themselves is infinite (Precup, Sutton, and\n",
    "Dasgupta 2001). In practice, the weighted estimator usually has dramatically lower\n",
    "variance and is strongly preferred.\n",
    "    - I don't really understand why the weight importance sampling is biased and the ordinary version is not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56941cacf15e8b05765996006082865469347c2b4cdce983108d1335de8b4245"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
