{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 5 Monte Carlo Methods\n",
    "- Require on experience to learn and do not need a complete description of the environment like Dynamic Programming policy evaluation and improvement algorithms did from the previous chapter. \n",
    "- Learning from actual experience is striking because it requires no prior knowledge of the environment’s dynamics, yet can still attain optimal behavior. Learning  from simulated experience is also powerful. \n",
    "-  To ensure that well-defined returns are available, here we define Monte Carlo methods only for episodic tasks. That is, we assume experience is divided into episodes, and that all episodes eventually terminate no matter what actions are  selected. Only on the completion of an episode are value estimates and policies changed\n",
    "- The term “Monte Carlo” is often used more broadly for any estimation method whose operation involves a significant random component. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 Monte Carlo Prediction \n",
    "- Value of a state is the expected return\n",
    "- One obvious way to compute is average returns. \n",
    "- First Visis and every-visit monte carlo methods. \n",
    "    - First visit only uses 1 first return for each state visited in an episode\n",
    "    - Every visit averages returns from re-visited states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 Monte Carlo Estimation of Action Values\n",
    "\n",
    "- The every-visit MC method estimates the value of a state–action pair as the average of the returns that have followed all the visits to it. The first-visit MC method averages the returns following the first time in each episode that the state was visited and the action was selected. These methods converge quadratically, as before, to the true expected values as the number of visits to each state–action pair approaches infinity.\n",
    "- The only complication is that many state–action pairs may never be visited. If ⇡ is a deterministic policy, then in following ⇡ one will observe returns only for one of the actions from each state. With no returns to average, the Monte Carlo estimates of the other actions will not improve with experience. \n",
    "- We need to **maintain exploration**. \n",
    "- One way to do this is by specifying that the episodes start in a **state–action pair**, and that every pair has a nonzero probability of being selected as the start. This guarantees that all state–action pairs will be visited an infinite number of times in the limit of an infinite number of episodes. We call this the assumption of **exploring starts**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 Monte Carlo Control\n",
    "- How is Monte Carlo used to aproximate optimal policies? \n",
    "    - Uses GPI\n",
    "        - Evaluate the policy then improve \n",
    "        - The value function is repeatedly altered to more closely approximate the value function for the current policy, and the policy is repeatedly improved with respect to the current value function\n",
    "        ![](images/GPI-Monte-Carlo.png)\n",
    "\n",
    "- Policy evaluation is done exactly as described in the preceding section. Many episodes are experienced, with the approximate action-value function approaching the true function asymptotically. \n",
    "\n",
    "- Policy improvement from 4.2 applies here as well. \n",
    "\n",
    "![](images/Monte-Carlo-Policy-Improvement.png)\n",
    "\n",
    "\n",
    "- We made two unlikely assumptions above in order to easily obtain this guarantee of convergence for the Monte Carlo method. One was that the episodes have exploringstarts, and the other was that policy evaluation could be done with an infinite number of episodes. To obtain a practical algorithm we will have to remove both assumptions. We postpone consideration of the first assumption until later in this chapter. For now we focus on the assumption that policy evaluation operates on an infinite number of episodes. This assumption is relatively easy to remove. In fact, the same issue arises even in classical DP methods such as iterative policy evaluation, which also converge only asymptotically to the true value function.\n",
    "\n",
    "- There is a second approach to avoiding the infinite number of episodes nominally required for policy evaluation, in which we give up trying to complete policy evaluation before returning to policy improvement. On each evaluation step we move the value function toward q⇡k , but we do not expect to actually get close except over many steps. We used this idea when we first introduced the idea of GPI in Section 4.6. One extreme form of the idea is value iteration, in which only one iteration of iterative policy evaluation is performed between each step of policy improvement. The in-place version of value iteration is even more extreme; there we alternate between improvement and evaluation steps for single states.\n",
    "\n",
    "- For Monte Carlo policy iteration it is natural to alternate between evaluation and\n",
    "improvement on an episode-by-episode basis.\n",
    "\n",
    "![](images/Monte-Carlo-ES.png)\n",
    "\n",
    "- Convergence to this optimal fixed point seems inevitable as the changes to the action-value function decrease over time, but has not yet been formally proved. In our opinion, this is one of the most fundamental open theoretical questions in reinforcement learning (for a partial solution, see Tsitsiklis, 2002)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56941cacf15e8b05765996006082865469347c2b4cdce983108d1335de8b4245"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
