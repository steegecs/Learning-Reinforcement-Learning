{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaper 3 Finite Markov Decision Processes\n",
    "    - Classical formalization of sequential decision making. \n",
    "    - Actions are not just influenced by immediate rewards, but also move sequences, or states, and future rewards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Agent and Environment Interface\n",
    "- Decision make is the agent\n",
    "- Everything else is the environment\n",
    "\n",
    "![](images/Agent-Environment%20Interaction.png)\n",
    "\n",
    "- In finite Markov decision processes, there is a finite set of states actions and rewards. \n",
    "- The R (Reward) and S (State) at time t all have defined probability distrubutions dependent only on the preceding state and action.\n",
    "\n",
    "![](images/state-reward-probability.png) \n",
    "\n",
    "- The function above defines the dynamics of the MDP.\n",
    "\n",
    "![](images/Markov-Probability-Theorem.png)\n",
    "\n",
    "- \"In a Markov decision process, the probabilities given by p completely characterize the\n",
    "environment’s dynamics.\"\n",
    "- \"The state must include information about all aspects\n",
    "of the past agent–environment interaction that make a di↵erence for the future. If it\n",
    "does, then the state is said to have the Markov property\".\n",
    "    - This markov property is often assumed at the model level\n",
    "    \n",
    "- THE MARKOV PROTPERTY IS ASSUMED THROGHOUT THE BOOK.\n",
    "\n",
    "- All sensory input can be a part of the environment. \n",
    "\n",
    "- Agent may have absolute control over its environment but not knowledge of it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Goals and Rewards.\n",
    " - Reward Hypothesis\n",
    "    - That all of what we mean by goals and purposes can be well thought of as\n",
    "    the maximization of the expected value of the cumulative sum of a received\n",
    "    scalar signal (called reward)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Returns and Eposodes\n",
    "- We seek to maximize expected rewards accross games. \n",
    "- Some takes however are **continueing** tasks. There is not a defined termination of a task. \n",
    "\n",
    "- Rewards in a game can be discounted... \n",
    "\n",
    "![](images/Discounting.png)\n",
    "\n",
    "- A very small gamma value makes the agent myopic... valuing immediate rewards more. Or more farsighted with a high gamma.\n",
    "\n",
    "![](images/Discounting-Games.png)\n",
    "\n",
    "![](images/Finite-Rewards-Discounting.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 Unified Notation for Episodic and Continueing Tasks\n",
    "- In episodic tasks, it is common that people just drop the episode number on state, aciton, and reward variables. \n",
    "- Because episodic and continuing tasks are very similar we can unify the notion. \n",
    "\n",
    "![](images/Unified-Notation.png)\n",
    "\n",
    "- T can be either infinate or a terminal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5 Policies and Value Functions\n",
    "- Almost all reinforcement learning uses **Value Functions** .\n",
    "    - How good it is to be in a state or perform an action in a state.\n",
    "\n",
    "- Reinforcement learning methods specify how the agent's policy is changed as a result of its experience. \n",
    "\n",
    "- **state-value function**\n",
    "\n",
    "![](images/Policy-Value.png)\n",
    "\n",
    "- **action-value function**\n",
    "\n",
    "![](images/Action-Value.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo Methods\n",
    "- Use of averaging over many random samples of actual returns to assess the value of a state or action.\n",
    "\n",
    "- A fundamental property of value functions used throughout reinforcement learning and dynamic programming is that they satisfy **recursive relationships** similar to that which we have already established for the return (3.9)\n",
    "\n",
    "- Equation (3.14) is the **Bellman equation** for v⇡. It expresses\n",
    "a relationship between the value of a state and the values of\n",
    "its successor states. \n",
    "\n",
    "![](images/Consistency-Condition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.19 The value of an action, q⇡(s, a), depends on the expected next reward and the expected sum of the remaining rewards. Again we can think of this in terms of a small backup diagram, this one rooted at an action (state–action pair) and branching to the possible next states:\n",
    "\n",
    "![](images/exercise-19.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56941cacf15e8b05765996006082865469347c2b4cdce983108d1335de8b4245"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
