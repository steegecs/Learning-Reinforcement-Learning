{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.2 Is the MDP framework adequate to usefully represent all goal-directed\n",
    "learning tasks? Can you think of any clear exceptions?\n",
    "- I think a clear exception to this would be in the case where many actions and states are unknown to the agent, or if the reward signal is hidden from the agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.3 Consider the problem of driving. You could define the actions in terms of\n",
    "the accelerator, steering wheel, and brake, that is, where your body meets the machine.\n",
    "Or you could define them farther out—say, where the rubber meets the road, considering\n",
    "your actions to be tire torques. Or you could define them farther in—say, where your\n",
    "brain meets your body, the actions being muscle twitches to control your limbs. Or you\n",
    "could go to a really high level and say that your actions are your choices of where to drive.\n",
    "What is the right level, the right place to draw the line between agent and environment?\n",
    "On what basis is one location of the line to be preferred over another? Is there any\n",
    "fundamental reason for preferring one location over another, or is it a free choice?\n",
    "- I don't think it is purely a free choice. I think it makes sense to make the action space contain the things an agent has voluntary control over. Other things that are out of the agents control should be a part of the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.4 Give a table analogous to that in Example 3.3, but for p(s0\n",
    ", r|s, a). It\n",
    "should have columns for s, a, s0\n",
    ", r, and p(s0\n",
    ", r|s, a), and a row for every 4-tuple for which\n",
    "p(s0\n",
    ", r|s, a) > 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.5 The equations in Section 3.1 are for the continuing case and need to be\n",
    "modified (very slightly) to apply to episodic tasks. Show that you know the modifications\n",
    "needed by giving the modified version of (3.3).\n",
    "- For the eposidic version of these equations, we should include the terminal state and not from the set of all states. \n",
    "\n",
    "![](images/episodic%20states.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.6 Suppose you treated pole-balancing as an episodic task but also used\n",
    "discounting, with all rewards zero except for 1 upon failure. What then would the\n",
    "return be at each time? How does this return differ from that in the discounted, continuing\n",
    "formulation of this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.7 Imagine that you are designing a robot to run a maze. You decide to give it a\n",
    "reward of +1 for escaping from the maze and a reward of zero at all other times. The task\n",
    "seems to break down naturally into episodes—the successive runs through the maze—so\n",
    "you decide to treat it as an episodic task, where the goal is to maximize expected total\n",
    "reward (3.7). After running the learning agent for a while, you find that it is showing\n",
    "no improvement in escaping from the maze. What is going wrong? Have you e↵ectively\n",
    "communicated to the agent what you want it to achieve?\n",
    "\n",
    "- The agent may not be able to find its way out of the maze for the first time so it does not know there is a way to maximize rewards. \n",
    "- Ways that you could handle this are by including a penality for staying in the maze, and seeding it well to show that there is a path out of the maze. It might not experience any urgency in finding its way out of the maze because there is no penalty. The actual behavior it not really being rewarded.\n",
    "- all you are telling it was that the last action taken was a good action (the action where it left the maze). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.8 Suppose  = 0.5 and the following sequence of rewards is received R1 = 1,\n",
    "R2 = 2, R3 = 6, R4 = 3, and R5 = 2, with T = 5. What are G0, G1, ..., G5? Hint:\n",
    "Work backwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G(5) = 0\n",
    "G(4) = 2\n",
    "G(3) = 4\n",
    "G(2) = 8\n",
    "G(1) = 6\n",
    "G(8) = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.9 Suppose  = 0.9 and the reward sequence is R1 = 2 followed by an infinite\n",
    "sequence of 7s. What are G1 and G0?\n",
    "\n",
    "G(1) = sum(i to infinity) {(9/10)^i * 7} = 70, G(0) = 2 + .9 * 70 = 65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.10 Prove the second equality in (3.10).\n",
    "- Just prove the geometric series and you got it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.11 If the current state is St, and actions are selected according to a stochastic\n",
    "policy ⇡, then what is the expectation of Rt+1 in terms of ⇡ and the four-argument\n",
    "function p (3.2)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.12 Give an equation for v⇡ in terms of q⇡ and ⇡. ⇤\n",
    "\n",
    "![](images/Exercise-3.12.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.14 The Bellman equation (3.14) must hold for each state for the value function v⇡ shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighboring states, valued at +2.3, +0.4, 0.4, and +0.7. (These numbers are accurate only to one decimal place.)\n",
    "\n",
    "(2.3 + 0.7 + 0.4 + (-0.4)) * .9 = 0.675"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.16 Now consider adding a constant c to all the rewards in an episodic task, such as maze running. Would this have any effect, or would it leave the task unchanged. as in the continuing task above? Why or why not? Give an example\n",
    "\n",
    "- Adding constants in an episodic task could definitely have an impact on the results. The reason for this is due to it terminating at some point. Imagine a situation where you are in a maze and there is a cost of -1 for each step and a +1 at the terminal state. The best option is to take the shortest past to termination. If you instead add just 2 to both of these. then the agent will want to take the longest path to collect rewards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.17 What is the Bellman equation for action values, that\n",
    "is, for q⇡? It must give the action value q⇡(s, a) in terms of the action\n",
    "values, q⇡(s0, a0), of possible successors to the state–action pair (s, a).\n",
    "Hint: The backup diagram to the right corresponds to this equation.\n",
    "Show the sequence of equations analogous to (3.14), but for action\n",
    "values.\n",
    "\n",
    "![](images/exercise-3.17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.18 The value of a state depends on the values of the actions possible in that state and on how likely each action is to be taken under the current policy. We can think of this in terms of a small backup diagram rooted at the state and considering each possible action:\n",
    "\n",
    "![](images/exercise-3-18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.20 Draw or describe the optimal state-value function for the golf example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use a driver to get to the green and then finish the hole with a putter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.21 Draw or describe the contours of the optimal action-value function for\n",
    "putting, q⇤(s, putter), for the golf example.\n",
    "\n",
    "- The optimal action value is the same as the optimal state value since there is only one aciton to choose from (the putter). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56941cacf15e8b05765996006082865469347c2b4cdce983108d1335de8b4245"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
