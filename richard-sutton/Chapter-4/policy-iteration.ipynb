{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A wrapper class for parameters of the algorithm\n",
    "class Params:\n",
    "    def __init__(self):\n",
    "        # Max number of cars for each location\n",
    "        self.max_car = 20\n",
    "\n",
    "        # Max number of cars to move each night\n",
    "        self.max_move = 5\n",
    "\n",
    "        # Reward given to rent a car\n",
    "        self.reward_per_car = 10\n",
    "\n",
    "        # Cost to keep more car than half the maximum overnight, for the modified version of Jack's Car Rental problem\n",
    "        self.cost_per_slot_night = 4\n",
    "\n",
    "        # Cost to move a car\n",
    "        self.cost_per_car = 2\n",
    "\n",
    "        # Small number determining the accuracy of policy evaluation's estimation\n",
    "        self.theta = 0.01\n",
    "\n",
    "        # Discount value\n",
    "        self.gamma = 0.9\n",
    "\n",
    "        # Expectation for rental requests in first location\n",
    "        self.lambda_request_first = 3\n",
    "\n",
    "        # Expectation for rental requests in second location\n",
    "        self.lambda_request_second = 4\n",
    "\n",
    "        # Expectation for returns in first location\n",
    "        self.lambda_return_first = 3\n",
    "\n",
    "        # Expectation for returns in second location\n",
    "        self.lambda_return_second = 2\n",
    "\n",
    "        # Possible versions of the problem\n",
    "        self.problem_types = ['original_problem', 'modified_problem']\n",
    "\n",
    "\n",
    "class GamblersValueIteration():\n",
    "    def __init__(self, p_h, params):\n",
    "        # Set up the ph value\n",
    "        self.p_h = p_h\n",
    "\n",
    "        # Set up parameters\n",
    "        self.params = params\n",
    "\n",
    "        # All possible states\n",
    "        self.S = np.arange(1, self.params.max_money)\n",
    "\n",
    "        # Value function\n",
    "        self.V = np.zeros(self.params.max_money + 1)\n",
    "        self.V[0] = 0\n",
    "        self.V[self.params.max_money] = 1\n",
    "\n",
    "        # List of value functions\n",
    "        self.Vs = []\n",
    "\n",
    "        # Policy function\n",
    "        self.pi = None\n",
    "\n",
    "        # Number of sweeps needed to complete the problem\n",
    "        self.sweep_count = None\n",
    "\n",
    "    def solve_problem(self):\n",
    "        \"\"\"\n",
    "        Resolve Gambler Problem using Value Iteration\n",
    "        \"\"\"\n",
    "        self.sweep_count = 0\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in self.S:\n",
    "                v = self.V[s]\n",
    "                self.V[s] = np.max([self.V_eval(s, a) for a in self.A(s)])\n",
    "                delta = np.maximum(delta, abs(v - self.V[s]))\n",
    "            if self.sweep_count < 3:\n",
    "                self.Vs.append(self.V.copy())\n",
    "            self.sweep_count += 1\n",
    "            if delta < self.params.theta:\n",
    "                break\n",
    "        print('Sweeps needed:', self.sweep_count)\n",
    "        self.Vs.append(self.V.copy())\n",
    "        self.pi = [self.A(s)[np.argmax([self.V_eval(s, a) for a in self.A(s)])] for s in self.S]\n",
    "    \n",
    "    def A(self, s):\n",
    "        \"\"\"\n",
    "        Get all possible actions given a state\n",
    "        :param s: state\n",
    "        :return: possible actions\n",
    "        \"\"\"\n",
    "        # All possible actions\n",
    "        return np.arange(1, np.minimum(s, self.params.max_money - s) + 1)\n",
    "\n",
    "    def V_eval(self, s, a):\n",
    "        \"\"\"\n",
    "        Compute value given a state and an action for the state following the formula:\n",
    "        sum over all s',r of p(s',r|s, a)[r + gamma*V(s')]\n",
    "        :param s: state\n",
    "        :param a: action\n",
    "        :return: value\n",
    "        \"\"\"\n",
    "        return self.params.gamma * self.V[s + a] * self.p_h + self.params.gamma * self.V[s - a] * (1 - self.p_h)\n",
    "\n",
    "# rents out car and is credited with $10 by company\n",
    "# if he is out of cars then the business is lost\n",
    "# cars can be rented the day after returned \n",
    "# jack can move cars between locations for 2 dollars\n",
    "\n",
    "# Iterative Policy Evaluation Algorithm\n",
    "class PolicyIteration(): \n",
    "    def __init__(self, init_state_values, starting_actions, starting_state, terminal_state, state_x_size, state_y_size):\n",
    "        #Initialize state values randomly\n",
    "        self.evaluated_state_values = init_state_values\n",
    "\n",
    "        #discount factor\n",
    "        self.discount = 0.9\n",
    "        self.move_action_cost = -2.0 \n",
    "        self.max_move_size = 5\n",
    "        self.rental_cost = 10.0\n",
    "\n",
    "        self.return_lambda = [3, 2]\n",
    "        self.rent_lambda = [3, 4]\n",
    "\n",
    "        #Termination States\n",
    "        self.terminal_state = terminal_state\n",
    "\n",
    "        #When to stop\n",
    "        self.termination_diff = 0.00000001\n",
    "        self.diff = 100\n",
    "\n",
    "        # select random integer between 0 and 3\n",
    "        self.current_state = starting_state\n",
    "        self.current_action = starting_actions\n",
    "\n",
    "        self.state_x_size = state_x_size\n",
    "        self.state_y_size = state_y_size\n",
    "\n",
    "        self.action_probabilitiy = 1\n",
    "    \n",
    "    def get_starting_state_random(self):\n",
    "        x = np.random.randint(0, self.state_x_size)\n",
    "        y = np.random.randint(0, self.state_y_size)\n",
    "        while (x == self.terminal_state or y == self.terminal_state):\n",
    "            x = np.random.randint(0, self.state_x_size)\n",
    "            y = np.random.randint(0, self.state_y_size)\n",
    "        \n",
    "        return [x, y]\n",
    "\n",
    "    def get_action_cost(self, action): \n",
    "        return action[1] * self.move_action_cost\n",
    "\n",
    "    # Get the estimated state values using the action reward and \n",
    "    def get_next_state_value(self, state, action):\n",
    "        expected_value = 0.0\n",
    "\n",
    "        state_after_action = self.get_state_after_action(state, action)\n",
    "\n",
    "        rental_rewards = self.get_rental_rewards_next_day(state_after_action)\n",
    "        move_cost = self.get_action_cost(action)\n",
    "\n",
    "        net_reward = rental_rewards + move_cost\n",
    "\n",
    "        # print(rental_rewards, move_cost, net_reward, self.evaluated_state_values[state_after_action[0]][state_after_action[1]], self.action_probabilitiy * (net_reward + self.discount * self.evaluated_state_values[state_after_action[0]][state_after_action[1]]))\n",
    "\n",
    "        return self.action_probabilitiy * (net_reward + self.discount * self.evaluated_state_values[state_after_action[0]][state_after_action[1]])\n",
    "\n",
    "    def get_rental_rewards_next_day(self, state):\n",
    "        rental_earnings = 0.0\n",
    "\n",
    "        if state[0] >= self.return_lambda[0]: \n",
    "            rental_earnings += self.rent_lambda[0] * self.rental_cost\n",
    "        else: \n",
    "            rental_earnings += state[0] * self.rental_cost\n",
    "        \n",
    "        if state[1] >= self.return_lambda[1]:\n",
    "            rental_earnings += self.rent_lambda[1] * self.rental_cost\n",
    "        else:\n",
    "            rental_earnings += state[1] * self.rental_cost\n",
    "\n",
    "        return rental_earnings\n",
    "    \n",
    "    def get_state_after_action(self, state, action):\n",
    "        new_state = state\n",
    "\n",
    "        x_max_receive = min(self.state_x_size - state[0], action[1])\n",
    "        y_max_receive = min(self.state_y_size - state[1], action[1])\n",
    "\n",
    "        x_max_move = min(state[0], action[1])\n",
    "        y_max_move = min(state[1], action[1])\n",
    "\n",
    "        #Update based on move\n",
    "        if action[0] == 0:\n",
    "            new_state = [state[0] - min(x_max_move, y_max_receive), state[1] + min(x_max_move, y_max_receive)]\n",
    "        else: \n",
    "            new_state = [state[0] + min(x_max_receive, y_max_move), state[1] - min(x_max_receive, y_max_move)]\n",
    "    \n",
    "        # update the state based on the net value of rentals and returns\n",
    "        net_rentals_x = self.return_lambda[0] - self.rent_lambda[0]\n",
    "        net_rentals_y = self.return_lambda[1] - self.rent_lambda[1]\n",
    "        \n",
    "        if (net_rentals_x + new_state[0] > self.state_x_size):\n",
    "            new_state[0] = self.state_x_size\n",
    "        elif (net_rentals_x + new_state[0] < 0): \n",
    "            new_state[0] = 0\n",
    "        else: \n",
    "            new_state[0] = net_rentals_x + new_state[0]\n",
    "        \n",
    "        if (net_rentals_y + new_state[1] > self.state_y_size):\n",
    "            new_state[1] = self.state_y_size\n",
    "        elif (net_rentals_y + new_state[1] < 0):\n",
    "            new_state[1] = 0\n",
    "        else:\n",
    "            new_state[1] = net_rentals_y + new_state[1]\n",
    "        \n",
    "        return new_state\n",
    "    \n",
    "    def iterative_evaluation(self):\n",
    "        while (self.diff > self.termination_diff):\n",
    "            self.diff = 0.0\n",
    "\n",
    "            for i in range(self.state_x_size):\n",
    "                for j in range(self.state_y_size):\n",
    "                    self.current_state = [i, j]\n",
    "                    self.evaluated_state_values[i][j] = self.get_next_state_value(self.current_state, self.current_action[self.current_state[0]][self.current_state[1]])\n",
    "                    self.diff = max(self.diff, abs(self.evaluated_state_values[i][j] - self.evaluated_state_values[self.current_state[0]][self.current_state[1]]))\n",
    "                    # print(self.current_state)\n",
    "        \n",
    "        print(self.evaluated_state_values)\n",
    "    \n",
    "    def policy_improvement(self): \n",
    "        for i in range(self.state_x_size):\n",
    "            for j in range(self.state_y_size):\n",
    "                self.current_state = [i, j]\n",
    "                action = self.get_action_with_max_value(self.current_state)\n",
    "                self.current_action[i][j] = self.get_action_with_max_value(self.current_state)\n",
    "                # print(self.current_state)\n",
    "    \n",
    "    def get_action_with_max_value(self, state):\n",
    "        max_value = -1000000.0\n",
    "        max_action = [0, 0]\n",
    "        for i in range(self.max_move_size + 1):\n",
    "            action = [0, i]\n",
    "            new_state = self.get_state_after_action(state, action)\n",
    "\n",
    "            if (self.evaluated_state_values[new_state[0]][new_state[1]] > max_value):\n",
    "                max_value = self.evaluated_state_values[new_state[0]][new_state[1]]\n",
    "                max_action = action\n",
    "        \n",
    "        for i in range(self.max_move_size + 1):\n",
    "            action = [1, i]\n",
    "            new_state = self.get_state_after_action(state, action)\n",
    "\n",
    "            if (self.evaluated_state_values[new_state[0]][new_state[1]] > max_value):\n",
    "                max_value = self.evaluated_state_values[new_state[0]][new_state[1]]\n",
    "                max_action = action\n",
    "        \n",
    "        return max_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Problem\n",
    "terminal_state = 0\n",
    "lot_one_size = 20\n",
    "lot_two_size = 20\n",
    "\n",
    "x = np.random.randint(0, lot_one_size+1)\n",
    "y = np.random.randint(0, lot_two_size+1)\n",
    "while ([x, y] == terminal_state):\n",
    "    x = np.random.randint(0, lot_one_size+1)\n",
    "    y = np.random.randint(0, lot_two_size+1)\n",
    "\n",
    "starting_state = [x, y]\n",
    "\n",
    "#Initialize a random number\n",
    "init_state_values = np.random.rand(lot_one_size+1, lot_two_size+1)*100\n",
    "init_state_values[terminal_state, :] = 0\n",
    "init_state_values[:, terminal_state] = 0\n",
    "\n",
    "starting_actions = np.zeros((lot_one_size+1, lot_two_size+1, 2), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives you the expexted value of a state in the grid world\n",
    "policy = PolicyIteration(init_state_values, starting_actions, starting_state, terminal_state, lot_one_size, lot_two_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.policy_improvement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.iterative_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.policy_improvement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.iterative_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56941cacf15e8b05765996006082865469347c2b4cdce983108d1335de8b4245"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
