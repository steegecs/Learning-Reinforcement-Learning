{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Concepts: \n",
    "- GPI (Generalized Policy Iteration)\n",
    "    - Iteratively approximating and improving to the optimal value function\n",
    "    - Feedback between evaluation and improvement\n",
    "    - Evaluate with a Bellman equation lookup.\n",
    "    - Coinflip Example - Remember the policy where it learns to bet it all at 25 and 50? \n",
    "- In-Place Dynamic Programming\n",
    "- Asynchronous Dynamic Programming. \n",
    "    - Update the state value of a subset of states. Problems can be unsolveable otherways with too large of search spaces. \n",
    "- Concrete Resource Allocation Problem\n",
    "- Policy Iteration Vs. Value Iteration\n",
    "    - Value iteration just takes one loop before policy update. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 4 Dynamic Programming\n",
    "\n",
    "- Refers to algoriths that can be used to compute optimal policies given a perfect model of the environment. \n",
    "    - Assumed a finite MDP (Markov Decision Process)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Policy Evaluation\n",
    "\n",
    "- Below is a formalization of DP iterative policy evaluation. Notice how the next value is being updated by the next state under the current value. You can use this to update the value of a state under policy changes. \n",
    "\n",
    "![](images/DP-Iterative-Update.png)\n",
    "\n",
    "\n",
    "- Iterative Policy Evaluation converges in the limit because eventually the random starting valuation starts to converge to zero after you loop through all of the actions, and update the values over and over again.\n",
    "\n",
    "![](images/Iterative-Policy-Evalutation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 4.4 Value Iteration\n",
    "\n",
    "![](images/Value-Iteration.png)\n",
    "\n",
    "\n",
    "- Update that value of a state based by setting the value to the expected value of your best action\n",
    "\n",
    "![](images/Value-Iteration-Algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5 Asynchronoous Dynamic Programming \n",
    "- These dynamic programming solution to state and action evaluation and policy updates are very computationally expensive, as you are continually sweeping through the whole state and actions space. It is possible however, to include methods for truncate some of this process so that you do not have to sweep through all states and actions to make an evaluation. We can also do things like skipping states we know are not relevant for optimal behavior or order the sweep through states to propagate value updates more efficiently. \n",
    "- With asynchronous dynamic programming, we can run an iterative DP algorithm while at the same time the agent is experiencing the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.6 Generalized Policy Iteration\n",
    "- Policy iteration is made up of two main processes\n",
    "    - policy evaluation\n",
    "    - policy improvement\n",
    "- The term for the policy evaluation and improvement interaction is the generalized policy iteration (GPI)\n",
    "    - Almost all RL problems are described as GPI.\n",
    "- The value function stabilizes only when it\n",
    "is consistent with the current policy, and the policy stabilizes\n",
    "only when it is greedy with respect to the current value function.\n",
    "- The evaluation and improvement processes in GPI can be viewed as both competing\n",
    "and cooperating. They compete in the sense that they pull in opposing directions. Making\n",
    "the policy greedy with respect to the value function typically makes the value function\n",
    "incorrect for the changed policy, and making the value function consistent with the policy\n",
    "typically causes that policy no longer to be greedy.\n",
    "- The arrows in this diagram correspond to the behavior of policy iteration in that each\n",
    "takes the system all the way to achieving one of the two goals completely. In GPI\n",
    "one could also take smaller, incomplete steps toward each goal. In either case, the two\n",
    "processes together achieve the overall goal of optimality even though neither is attempting\n",
    "to achieve it directly\n",
    "\n",
    "![](images/Evaluation-Improvemnt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.7 Efficiency of Dynamic Programming\n",
    "- Worst case it take polynomial time to solve a dynamic programming problem. \n",
    "- If n and k denote the number of states\n",
    "and actions, this means that a DP method takes a number of computational operations\n",
    "that is less than some polynomial function of n and k. A DP method is guaranteed to\n",
    "find an optimal policy in polynomial time even though the total number of (deterministic)\n",
    "policies is kn. In this sense, DP is exponentially faster than any direct search in policy\n",
    "space could be. \n",
    "- For the largest problems, only DP methods are feasible.\n",
    "- On problems with large state spaces, asynchronous DP methods are often preferred. To\n",
    "complete even one sweep of a synchronous method requires computation and memory for\n",
    "every state. For some problems, even this much memory and computation is impractical,\n",
    "yet the problem is still potentially solvable because relatively few states occur along\n",
    "optimal solution trajectories. Asynchronous methods and other variations of GPI can be\n",
    "applied in such cases and may find good or optimal policies much faster than synchronous\n",
    "methods can.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.8 Summary\n",
    "- Policy improvement refers to the computation of an improved policy given the value function for that policy. Putting these two computations together, we obtain policy iteration and value iteration, the two most popular DP methods. Either of these can be used to reliably compute optimal policies and value functions for finite MDPs given complete knowledge of the MDP.\n",
    "- An intuitive view of the operation of DP updates is given by their backup diagrams.\n",
    "![](images/0_6UMWl8MxHQ071yxF.png)\n",
    "\n",
    "- One last special property of DP methods. All of them update estimates\n",
    "of the values of states based on estimates of the values of successor states. That is, they\n",
    "update estimates on the basis of other estimates. We call this general idea bootstrapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56941cacf15e8b05765996006082865469347c2b4cdce983108d1335de8b4245"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
