{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 4 Dynamic Programming\n",
    "\n",
    "- Refers to algoriths that can be used to compute optimal policies given a perfect model of the environment. \n",
    "    - Assumed a finite MDP (Markov Decision Process)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Policy Evaluation\n",
    "\n",
    "- Below is a formalization of DP iterative policy evaluation. Notice how the next value is being updated by the next state under the current value. You can use this to update the value of a state under policy changes. \n",
    "\n",
    "![](images/DP-Iterative-Update.png)\n",
    "\n",
    "\n",
    "- Iterative Policy Evaluation converges in the limit because eventually the random starting valuation starts to converge to zero after you loop through all of the actions, and update the values over and over again.\n",
    "\n",
    "![](images/Iterative-Policy-Evalutation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56941cacf15e8b05765996006082865469347c2b4cdce983108d1335de8b4245"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
