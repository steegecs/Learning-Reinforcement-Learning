{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Concepts: \n",
    "- n-step TD Learning: Unifying Monte Carlo and TD(0)\n",
    "    - TD(0) only evaluated a state using one step\n",
    "    - TD(infinity) = Monte Carlo\n",
    "    - Monte Carlo is noisy while TD is it is not sample efficient or computationally efficient.\n",
    "    - Best is often the intermediate between these two. \n",
    "    - \"All n-step returns can be considered approximations to the full return.\"\n",
    "    - How many steps do you take before doing TD update?\n",
    "    - Update state(t) = Bootstrap the discounted rewards + the discounted value state(t+n). \n",
    "    - KEY: Off-Policy Learning with Importance Sampling  \n",
    "- n-step SARSA (On-Policy)\n",
    "- n-step Expected SARSA (Off-Policy)\n",
    "- n-step Tree Backup (Off-Policy)\n",
    "- Q(o) - Q sigma algorithm (Off-Policy)\n",
    "    - Mixes taking the expectation and importance sampling approach to value updates.\n",
    "    - On one step it may use expected sarsa to get the value, and on the next it uses Importance sample to balance with the target/behavior policy weighting paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 7 - n-step Bootstrapping"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
