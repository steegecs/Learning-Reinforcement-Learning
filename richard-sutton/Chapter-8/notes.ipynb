{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Concepts: \n",
    "- Unifying Planning and Learning (Model-Based / Model-Free)\n",
    "    - Model - In Chapter 4 we saw DP algorithms using a Markov Decision Process (MDP) where we have a Model of the environment (i.e The dynamics are completely know - we know the transition probabilities of the next state and reward given current state-action pairs)\n",
    "    - Model-Free - Monte Carlo and TD learning - They learn from experience\n",
    "- Random-sample one-step tabular Q-planning\n",
    "    - Send state and action to a model and sample next reward R and next state S^\n",
    "- Planning steps allow you to learn a reward state before you actually reach it in action.  \n",
    "- Dyna Q vs. Q\n",
    "    - Uses something like UCB - Adds a reward to states that have not been experience in a while\n",
    "    - This style of learning allows you to learn optimal paths more often even in chaning environment.\n",
    "        -i.e. a maze that moves around. \n",
    "- Expected vs. Sample Updates\n",
    "    - Expected updates are much more comupationally expensive as it has to track multiple branches (see backup diagram)\n",
    "    - Instead of going through all possible next states and rewards for a given state action pair and multiplying the rewards by the probabilities - just choose one next state.\n",
    "- Prioritized Sweeping\n",
    "    - 3 concentric circles\n",
    "        - Start states\n",
    "        - Relevants States\n",
    "        - Irrelevant States\n",
    "            - Unreacheable from any start state under any optimal policy\n",
    "    - Keeps track of states that have had large changes in value updates? \n",
    "- Decision Time Planning\n",
    "    - Background planning\n",
    "    - Heuristic Search\n",
    "        - Computes values of many different trees and then takes an action\n",
    "    -Monte Carlo Tree Search\n",
    "        - Keeps track of the trees that you have search and repurposes them to save computation rather than having to re-compute trees as you go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 8 - Planning and Learning with Tabular Methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56941cacf15e8b05765996006082865469347c2b4cdce983108d1335de8b4245"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
