{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Concepts:\n",
    "- TD Learning: Cmbine experience with the Bellman equation\n",
    "    - Rather than requiring the sampel of a whole episodic task, you can bootstrap the update with a learned subsequent value state. \n",
    "    - TD(0) = 1 step look ahead (1 step bootstrap)\n",
    "    - GPI does a value iteration and policy update after each step taken \n",
    "- Driving Home Example\n",
    "    - We can make intermediate predictions mid-episode. This allows us to understand at a finer level what happened throughout the course of an episode. This allows us to update the value of a particular state, depending on a single transition, rather than all intermediate state updates being contingent upon the entire episode.\n",
    "- Batch updating\n",
    "    - Saving experiences and using it to update the value functions.\n",
    "- SARSA\n",
    "    - Selects the actions and value updates using the e-greedy policy (on-policy value iteration)\n",
    "- Q-LEARNING\n",
    "    - Selects the actions based on e-greedy policy, but value updates are done using the greedy action (off-policy value iteration).\n",
    "- Cliff walking example\n",
    "    - SARSA performs better because the Q-learning is learning with respect to the optimal value, and thus missing updating the state values of the random actions that could lead it off the cliff. \n",
    "- Expected SARSA\n",
    "    - Selects action using e-greedy mechanism, but value updates using the expected value of the the action (off-policy)\n",
    "- Double Q Learning\n",
    "    - Helps solves for Q learning's positivity bias (Cliff walking example). Might take a non-optimal action, yet it evaluated based on the optimal value. Q learning's policy iteration can mis-evaluate the situation, with respect to its policy, because it is off policy and has a positivity bias. \n",
    "    - Double Q learning uses two separate Q tables that are evaluated against each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 6 Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1 TD Prediction\n",
    "- TD and Monte Carlo both use exporience to solve a prediction problem.\n",
    "- We refer to TD and Monte Carlo updates as sample updates because they involve looking ahead to a sample successor states,\n",
    "- Sample updates differ from the expected updates of DP methods in that they are based on a single sample successor rather than on a complete distribution of all possible successors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarsa is like like Q learning except when evaluating a state, it randomly selects an action from the next state (instead of argmax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56941cacf15e8b05765996006082865469347c2b4cdce983108d1335de8b4245"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
